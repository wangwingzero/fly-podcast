name: compose

on:
  schedule:
    - cron: "30 19 * * *" # Beijing 03:30
  workflow_dispatch:

env:
  DRY_RUN: ${{ vars.DRY_RUN || 'true' }}
  TARGET_ARTICLE_COUNT: ${{ vars.TARGET_ARTICLE_COUNT || '10' }}
  DOMESTIC_RATIO: ${{ vars.DOMESTIC_RATIO || '0.6' }}
  QUALITY_THRESHOLD: ${{ vars.QUALITY_THRESHOLD || '80' }}
  ALLOW_GOOGLE_REDIRECT_CITATION: ${{ vars.ALLOW_GOOGLE_REDIRECT_CITATION || 'false' }}
  LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
  LLM_BASE_URL: ${{ vars.LLM_BASE_URL || 'https://api.agentify.top/v1/chat/completions' }}
  LLM_MODEL: ${{ vars.LLM_MODEL || 'openai/gpt-oss-120b' }}
  LLM_MAX_TOKENS: ${{ vars.LLM_MAX_TOKENS || '6000' }}
  LLM_TEMPERATURE: ${{ vars.LLM_TEMPERATURE || '0.1' }}

concurrency:
  group: flying-podcast-compose
  cancel-in-progress: false

jobs:
  compose:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - run: pip install -r requirements.txt
      - name: Download recent published history from R2
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_BUCKET: ${{ vars.R2_BUCKET || 'flying-podcast' }}
        run: |
          mkdir -p data/history
          aws s3 cp "s3://${R2_BUCKET}/history/recent_published.json" data/history/recent_published.json \
            --endpoint-url "${R2_ENDPOINT}" || echo "No history file yet, first run"
      - name: Ensure ranked data exists
        run: |
          python run.py ingest --date "$(date -u +%F)"
          python run.py rank --date "$(date -u +%F)"
      - name: Run compose
        run: python run.py compose --date "$(date -u +%F)"
      - name: Upload composed artifacts
        uses: actions/upload-artifact@v4
        with:
          name: composed-${{ github.run_id }}
          path: data/processed/composed_*.json
