name: podcast-from-pdf

on:
  push:
    branches: [main]
    paths:
      - "podcast_pdfs/**.pdf"
  workflow_dispatch:
    inputs:
      greeting:
        description: "Extra prompt for dialogue (e.g. holiday greeting)"
        required: false
        default: ""
      pdf_filter:
        description: "Only process PDFs matching this keyword (empty = all)"
        required: false
        default: ""

env:
  TZ: Asia/Shanghai
  LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
  LLM_BASE_URL: ${{ vars.LLM_BASE_URL || 'https://api.agentify.top/v1/chat/completions' }}
  LLM_MODEL: ${{ vars.LLM_MODEL || 'openai/gpt-oss-120b' }}
  LLM_MAX_TOKENS: ${{ vars.LLM_MAX_TOKENS || '6000' }}
  LLM_TEMPERATURE: ${{ vars.LLM_TEMPERATURE || '0.1' }}
  DASHSCOPE_API_KEY: ${{ secrets.DASHSCOPE_API_KEY }}
  QWEN_TTS_URL: ${{ vars.QWEN_TTS_URL || 'http://localhost:8825' }}
  PODCAST_GREETING: ${{ github.event.inputs.greeting || '' }}
  UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
  UNSPLASH_ACCESS_KEY_2: ${{ secrets.UNSPLASH_ACCESS_KEY_2 }}
  PIXABAY_API_KEY: ${{ secrets.PIXABAY_API_KEY }}
  IMAGE_GEN_API_KEY: ${{ secrets.IMAGE_GEN_API_KEY }}
  IMAGE_GEN_BASE_URL: ${{ vars.IMAGE_GEN_BASE_URL || '' }}
  IMAGE_GEN_MODEL: ${{ vars.IMAGE_GEN_MODEL || '' }}
  IMAGE_GEN_BACKUP_API_KEY: ${{ secrets.IMAGE_GEN_BACKUP_API_KEY }}
  IMAGE_GEN_BACKUP_BASE_URL: ${{ vars.IMAGE_GEN_BACKUP_BASE_URL || '' }}
  IMAGE_GEN_BACKUP_MODEL: ${{ vars.IMAGE_GEN_BACKUP_MODEL || '' }}

concurrency:
  group: podcast-from-pdf
  cancel-in-progress: false

jobs:
  generate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          sudo apt-get update && sudo apt-get install -y ffmpeg

      - name: Find PDFs to process
        id: find-pdfs
        run: |
          FILTER="${{ github.event.inputs.pdf_filter }}"
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            # Manual trigger: process ALL PDFs in the folder
            PDFS=$(find podcast_pdfs -maxdepth 1 -name "*.pdf" -type f 2>/dev/null || true)
          else
            # Push trigger: only process newly added PDFs
            PDFS=$(git diff --name-only --diff-filter=A ${{ github.event.before }} ${{ github.sha }} -- 'podcast_pdfs/*.pdf' || true)
          fi
          # Apply filter if specified
          if [ -n "$FILTER" ] && [ -n "$PDFS" ]; then
            PDFS=$(echo "$PDFS" | grep -i "$FILTER" || true)
          fi
          if [ -z "$PDFS" ]; then
            echo "No PDFs to process"
            echo "has_pdfs=false" >> "$GITHUB_OUTPUT"
          else
            echo "PDFs to process:"
            echo "$PDFS"
            echo "$PDFS" > /tmp/new_pdfs.txt
            echo "has_pdfs=true" >> "$GITHUB_OUTPUT"
          fi

      - name: Generate podcasts
        if: steps.find-pdfs.outputs.has_pdfs == 'true'
        run: |
          DATE=$(TZ=Asia/Shanghai date +%F)
          SUCCESS=0
          FAIL=0
          while IFS= read -r pdf; do
            [ -f "$pdf" ] || continue
            echo "=========================================="
            echo "Processing: $pdf"
            echo "=========================================="
            if python run.py podcast --date "$DATE" --pdf "$pdf"; then
              SUCCESS=$((SUCCESS + 1))
            else
              echo "::warning::Failed to process: $pdf"
              FAIL=$((FAIL + 1))
            fi
          done < /tmp/new_pdfs.txt
          echo "Done: $SUCCESS succeeded, $FAIL failed"
          if [ $SUCCESS -eq 0 ] && [ $FAIL -gt 0 ]; then
            echo "::error::All PDFs failed to process"
            exit 1
          fi

      - name: Upload podcasts to R2
        if: steps.find-pdfs.outputs.has_pdfs == 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_BUCKET: ${{ vars.R2_BUCKET || 'flying-podcast' }}
        run: |
          if [ -d data/output/podcast ]; then
            echo "Uploading podcast files to R2..."
            aws s3 sync data/output/podcast/ "s3://${R2_BUCKET}/podcast/" \
              --endpoint-url "${R2_ENDPOINT}"
            echo "Upload complete"
          else
            echo "No podcast output directory found"
          fi

      - name: Publish podcast drafts to WeChat
        if: steps.find-pdfs.outputs.has_pdfs == 'true'
        env:
          DRY_RUN: "false"
          WECHAT_ENABLE_PUBLISH: "true"
          WECHAT_APP_ID: ${{ secrets.WECHAT_APP_ID }}
          WECHAT_APP_SECRET: ${{ secrets.WECHAT_APP_SECRET }}
          WECHAT_THUMB_MEDIA_ID: ${{ secrets.WECHAT_THUMB_MEDIA_ID }}
          WECHAT_PROXY: ${{ vars.WECHAT_PROXY || '' }}
        run: |
          DATE=$(TZ=Asia/Shanghai date +%F)
          python run.py publish-podcast --date "$DATE"

      - name: Upload artifacts
        if: always() && steps.find-pdfs.outputs.has_pdfs == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: podcast-output-${{ github.run_id }}
          path: data/output/podcast/
